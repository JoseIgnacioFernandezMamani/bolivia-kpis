name: Scraper â€“ Daily Run

on:
  schedule:
    # Run every day at 04:00 UTC
    - cron: '0 4 * * *'
  workflow_dispatch:
    inputs:
      spider:
        description: 'Spider name to run (leave blank for all)'
        required: false
        default: ''

permissions:
  contents: read

jobs:
  run-scrapers:
    name: Run Scrapy Spiders
    runs-on: ubuntu-latest
    permissions:
      contents: read

    services:
      postgres:
        image: postgis/postgis:15-3.3
        env:
          POSTGRES_USER: bolivia
          POSTGRES_PASSWORD: ${{ secrets.POSTGRES_PASSWORD }}
          POSTGRES_DB: bolivia_kpis
        ports:
          - 5432:5432
        options: >-
          --health-cmd="pg_isready -U bolivia"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=5

      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd="redis-cli ping"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=5

    env:
      DATABASE_SYNC_URL: postgresql+psycopg2://bolivia:${{ secrets.POSTGRES_PASSWORD }}@localhost:5432/bolivia_kpis
      REDIS_URL: redis://localhost:6379/0
      SCRAPER_USER_AGENT: ${{ secrets.SCRAPER_USER_AGENT || 'BoliviaKPIs/1.0' }}

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip
          cache-dependency-path: backend/scraper/requirements.txt

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r backend/scraper/requirements.txt

      - name: Install Playwright browsers
        run: playwright install chromium --with-deps

      - name: Run all spiders
        if: github.event.inputs.spider == ''
        working-directory: backend/scraper
        run: python -m bolivia_scraper

      - name: Run specific spider
        if: github.event.inputs.spider != ''
        working-directory: backend/scraper
        run: python -m bolivia_scraper ${{ github.event.inputs.spider }}

      - name: Upload raw data artifact
        uses: actions/upload-artifact@v4
        with:
          name: raw-data-${{ github.run_id }}
          path: data/raw/
          retention-days: 7
